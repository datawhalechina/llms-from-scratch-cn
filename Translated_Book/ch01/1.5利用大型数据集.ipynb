{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 利用大型数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流行的GPT和BERT类模型的大型训练数据集代表了包含数十亿单词的多样化和全面的文本语料库，其中包括了广泛的主题以及自然语言和计算机语言。为了提供一个具体的例子，表1.1总结了用于预训练GPT-3的数据集，该数据集作为ChatGPT第一版的基础模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**表1.1 流行的GPT-3大型语言模型的预训练数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset name   | Dataset description   | Number of tokens | Proportion in training data |\n",
    "|----------------|-----------------------|------------------|-----------------------------|\n",
    "| CommonCrawl (filtered) | Web crawl data       | 410 billion      | 60%                         |\n",
    "| WebText2       | Web crawl data        | 19 billion       | 22%                         |\n",
    "| Books1         | Internet-based book corpus | 12 billion   | 8%                          |\n",
    "| Books2         | Internet-based book corpus | 55 billion   | 8%                          |\n",
    "| Wikipedia      | High-quality text     | 3 billion        | 3%                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表1.1报告了词符的数量，词符是模型读取的文本单位，数据集中的词符数量大致相当于文本中的单词和标点符号的数量。我们将在下一章更详细地介绍词符化，即将文本转换为词符的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要的结论是，这种训练数据集的规模和多样性使得这些模型能够在包括语言语法、语义和上下文在内的多种任务上表现良好，甚至一些需要通用知识的任务也能够胜任。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-3数据集详情**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在表1.1中，需要注意的是，从每个数据集中，只有一部分数据（总共3000亿个标记）被用于训练过程。这种抽样方法意味着训练并没有包括每个数据集中可用的每一条数据。相反，从所有数据集中抽取的选定的3000亿个标记的子集被利用。此外，虽然有些数据集在这个子集中没有被完全涵盖，但其他数据集可能被多次包括，以达到3000亿个标记的总数。表中指示比例的列，如果不考虑四舍五入的误差，加起来占了这个抽样数据的100%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了提供背景，考虑一下CommonCrawl数据集的大小，它单独就包含了4100亿个标记，需要大约570GB的存储空间。相比之下，像GPT-3这样的模型的后续版本，比如Meta的LLaMA，已经扩大了它们的训练范围，包括了额外的数据来源，如Arxiv研究论文（92GB）和StackExchange的代码相关问答（78GB）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia语料库由英语维基百科组成。虽然GPT-3论文的作者没有进一步指定细节，但Books1很可能是来自Project Gutenberg（https://www.gutenberg.org/)的样本，Books2很可能来自Libgen（https://en.wikipedia.org/wiki/Library_Genesis)。CommonCrawl是CommonCrawl数据库（https://commoncrawl.org/)的一个过滤子集，而WebText2是来自所有Reddit帖子中获得3个以上赞的外链网页的文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3论文的作者没有分享训练数据集，但一个公开可用的类似数据集是The Pile（https://pile.eleuther.ai/)。然而，这个集合可能包含受版权保护的作品，确切的使用条款可能取决于预期的用例和国家。更多信息，请参见HackerNews上的讨论，网址为 https://news.ycombinator.com/item?id=25607809。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些模型的预训练特性使它们在进一步微调下游任务时具有极高的灵活性，这也是它们被称为基础或底层模型的原因。预训练大型语言模型（LLMs）需要访问大量资源，并且成本非常高昂。例如，GPT-3的预训练成本估计为460万美元的云计算积分[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好消息是，许多预训练的大型语言模型（LLMs），作为开源模型提供，可以作为通用工具来编写、提取和编辑不是训练数据一部分的文本。此外，LLMs可以在相对较小的数据集上针对特定任务进行微调，从而减少所需的计算资源并提高在特定任务上的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这本书中，我们将实现预训练的代码，并使用它来出于教育目的预训练一个大型语言模型（LLM）。所有的计算都可以在消费者硬件上执行。在实现了预训练代码之后，我们将学习如何重用公开可用的模型权重，并将它们加载到我们将要实现的架构中，这允许我们在本书后面对LLMs进行微调时跳过昂贵的预训练阶段。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
