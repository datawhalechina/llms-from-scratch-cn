{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5bc148",
   "metadata": {},
   "source": [
    "# 5.6 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cbb4fc",
   "metadata": {},
   "source": [
    "- 大型语言模型（LLM）生成文本时，它们每次输出一个token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0f948",
   "metadata": {},
   "source": [
    "- 在默认情况下，通过将模型输出转换为概率分数，并选择对应于最高概率分数的词汇，来生成下一个token，这种方法称为“贪婪解码”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d231f",
   "metadata": {},
   "source": [
    "- 通过使用概率采样和温度缩放，我们可以影响生成文本的多样性和连贯性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2ffa4",
   "metadata": {},
   "source": [
    "- 在训练过程中，可以使用训练集和测试集的损失(loss)来衡量大语言模型生成文本的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cc1eb",
   "metadata": {},
   "source": [
    "- 预训练一个大语言模型涉及改变其权重的操作以最小化训练损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5979d",
   "metadata": {},
   "source": [
    "- 大语言模型的训练循环本身是深度学习中的标准程序，这个过程使用了常规的交叉熵损失和AdamW优化器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff620fb",
   "metadata": {},
   "source": [
    "- 由于在大型文本语料库上预训练大语言模型既耗时又耗资源，因此我们可以选择加载OpenAI的公开可用的权重，作为自己在大数据集上预训练模型的替代方案。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
