Mambaåœ¨ä¸€ä¸ªPyTorchæ–‡ä»¶ä¸­çš„ç®€å•ã€æç®€å®ç°ã€‚

ç‰¹ç‚¹ï¼š
* ä¸å®˜æ–¹å®ç°çš„å‰å‘å’Œåå‘ä¼ é€’å…·æœ‰ç›¸åŒçš„æ•°å€¼è¾“å‡º
* ç®€åŒ–çš„ã€å¯è¯»çš„ã€å¸¦æ³¨é‡Šçš„ä»£ç 

ä¸åŒ…æ‹¬ï¼š
* é€Ÿåº¦ã€‚å®˜æ–¹å®ç°ç»è¿‡å¤§é‡ä¼˜åŒ–ï¼Œè¿™äº›ä¼˜åŒ–æ˜¯Mambaè®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ä¹‹ä¸€ã€‚ä¸ºäº†å¯è¯»æ€§å°†å¤§éƒ¨åˆ†å®ç°ä¿æŒç®€å•ã€‚
* æ­£ç¡®çš„å‚æ•°åˆå§‹åŒ–ï¼ˆå°½ç®¡å¯ä»¥åœ¨ä¸ç‰ºç‰²å¯è¯»æ€§çš„æƒ…å†µä¸‹æ·»åŠ ï¼‰

## æ¼”ç¤º

å‚è§[demo.ipynb](demo.ipynb)ä»¥è·å–æç¤ºå®Œæˆçš„ç¤ºä¾‹ã€‚

```python
from model import Mamba
from transformers import AutoTokenizer

model = Mamba.from_pretrained('state-spaces/mamba-370m')
tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')

generate(model, tokenizer, 'Mamba is the')
```
> Mamba æ˜¯ä¸–ç•Œä¸Šæœ€é•¿çš„æ¯’è›‡ï¼Œä¼°è®¡é•¿åº¦è¶…è¿‡150ç±³ã€‚ç”±äºå…¶å·¨å¤§çš„ä½“å‹å’Œå‰§æ¯’çš„å’¬åˆåŠ›ï¼ŒMambaé€šè¿‡åˆºä¼¤å—å®³è€…æ¥æ€äººï¼ˆè¿™æ¯”å•æ¬¡å’¬åˆçš„åˆºç—›æ„Ÿæ›´å¼ºï¼Œä½†æ•ˆæœæ›´å·®ï¼‰

150ç±³â€¦â€¦ğŸ«¢ å¯æ€•ï¼

## å‚è€ƒèµ„æ–™

Mambaæ¶æ„ç”±[Albert Gu](https://twitter.com/_albertgu?lang=en)å’Œ[Tri Dao](https://twitter.com/tri_dao?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)åœ¨[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)ä¸­æå‡ºã€‚

å®˜æ–¹å®ç°è§æ­¤å¤„: https://github.com/state-spaces/mamba/tree/main